{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf8183cb-4376-4118-8fd0-c0ee53eed149",
   "metadata": {},
   "source": [
    "# **Introduction**\n",
    "\n",
    "This project aims to analyse prices and locations of airbnbs using airbnb data of Chicago as well as other data sources about chicago, mainly crime data.\n",
    "The following 12 suggested questions from the instructions should give an introduction to our project (in addition to the report):\n",
    "\n",
    "1. *Which dataset(s) did you choose? Why?* <br>\n",
    "We used essentially 3 types of data:\n",
    "* Airbnb **listings data** (each row is an airbnb with a number of attributes including geographic information via longitude and latitude values)\n",
    "* **Crime data** (each row is a crime incident with a number of attributes including geographic information via longitude and latitude values)\n",
    "* Chicago city data (we labeled this \"**population data**\"; it contains geographic data of chicago as geopandas polygons including population data -> this data serves as a common ground for merging airbnb and crime data).\n",
    "\n",
    "2. *How did you clean/transform the data? Why?* <br>\n",
    "Thankfully, most of the data was already quite \"clean\". The main challenge was obtaining the correct geospheric projection of longitude-latitude-pairs. In order to make meaningful joins of the data, they all need to be transformed to a commong projection (we used this one: https://epsg.io/26916). Also, in the listings data, one airbnb was listed with a prices of $9999999, which we had to get rid of for obvious reasons.\n",
    "\n",
    "Afterwards we only did variable selection and filtering of rows for the EDA and modeling steps.  \n",
    "\n",
    "3. *How did you solve the problem of missing values? Why?*\n",
    "\n",
    "The were barely any missing values, which we dropped. We didn't think this would impact the analyses as it was much less than 1%.\n",
    "\n",
    "4. *What questions did you ask of the data?*\n",
    "\n",
    "We essentially had two questions: <br>\n",
    "\n",
    "Q1: Can we analyse and predict the *locations* of Airbnbs within Chicago? <br>\n",
    "Q2: What are the influencing factors behind Airbnb prices and can we model this meaningfully? <br>\n",
    "\n",
    "5. *Why were these good questions?*\n",
    "\n",
    "While only focusing on Chicago, the insights could potentially generalize. They might be interesting to city planners or companies like Airbnb, and there is good free data available to tackle these questions. \n",
    "\n",
    "6. *What were the answers to these questions?*\n",
    "   \n",
    "7. *How did you obtain them?*\n",
    "8. *Do the answers make sense?*\n",
    "9. *Were there any difficulties in analysing the data? What were the key insights obtained?*\n",
    "10. *What are potential biases in the data and analysis?*\n",
    "11. *Which Data Science tools and techniques were learned during this exercise?* \n",
    "12. *How was the work divided up between the members of the group?*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773d79d8",
   "metadata": {},
   "source": [
    "# **Table of Contents**\n",
    "\n",
    "- **[1. Load and Process Data](#load-and-process-data)**\n",
    "    - **[1.1. Airbnb data](#airbnb-data)**\n",
    "    - **[1.2. Crime data](#crime-data)**\n",
    "        - **[1.2.1. Filter](#filter)**\n",
    "    - **[1.3. Population data](#population-data)**\n",
    "    - **[1.4. Additional City data](#additional-city-data)**\n",
    "- **[2. Exploratory Data Analysis (EDA)](#exploratory-data-analysis)**\n",
    "    - **[2.1. Geographic EDA](#geographic-eda)**\n",
    "    - **[2.2. Non-Geographic EDA](#non-geographic-eda)**\n",
    "- **[3. In-Depth Analysis & Modeling - Airbnb Locations](#in-depth-analysis-airbnb-locations)**\n",
    "- **[4. In-Depth Analysis & Modeling - Airbnb Prices](#in-depth-analysis-airbnb-prices)**\n",
    "    - **[4.1. K-means clustering](#k-means-clustering)**\n",
    "        - **[4.1.1. Data Processing steps](#data-processing-steps)**\n",
    "        - **[4.1.2. The Model](#the-model)**\n",
    "    - **[4.2. Associate Rule Learning (unsupervised co-occurence analysis)](#associate-rule-learning)**\n",
    "    - **[4.3. Supervised learning](#supervised-learning)**\n",
    "        - **[4.3.1. Data processing](#data-processing)**\n",
    "        - **[4.3.2. Train-test split](#train-test-split)**\n",
    "        - **[4.3.3. Choosing suitable ML](#choosing-suitable-ml)**\n",
    "        - **[4.3.4. Feature Selection](#feature-selection)**\n",
    "        - **[4.3.5. Scale features](#scale-features)**\n",
    "- **[5. Metrics & Evaluation](#metrics-evaluation)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a778974-898e-4b37-a640-36cba8636acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import geohash #--> !pip install python-geohash \n",
    "import gzip\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.patches as mpatches\n",
    "import seaborn as sns\n",
    "from shapely.geometry import Point, Polygon\n",
    "from shapely import wkt\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from plotnine import ggplot, aes, geom_point, geom_smooth, facet_wrap, geom_text, theme\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from geopy.geocoders import Nominatim\n",
    "from uszipcode import SearchEngine\n",
    "import plotly.express as px\n",
    "from sodapy import Socrata # crime data API\n",
    "import requests\n",
    "from io import StringIO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43a75303-7090-4498-a698-7e2d780b72b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### AIRBNB DATA ##########################################################################################################\n",
    "# source 1+2+3: http://insideairbnb.com/get-the-data/\n",
    "airbnb_data_path = \"./data/airbnb_data/\"\n",
    "listings_csv_path = airbnb_data_path + \"listings.csv.gz\"\n",
    "############################ CRIME DATA ##########################################################################################################\n",
    "crime_data_path = \"./data/crime_data/\"\n",
    "# source 1: https://data.cityofchicago.org/Public-Safety/Crimes-2001-to-Present/ijzp-q8t2\n",
    "response = requests.get('https://data.cityofchicago.org/resource/ijzp-q8t2.csv')\n",
    "assert response.status_code == 200\n",
    "data = response.content.decode('utf-8')\n",
    "df = pd.read_csv(StringIO(data))\n",
    "# df.to_csv(crime_data_path + \"Crimes_-_2001_to_Present.csv\") # uncomment to store data\n",
    "crime_csv_path = crime_data_path + \"Crimes_-_2001_to_Present.csv\"\n",
    "# source 2: https://data.cityofchicago.org/Public-Safety/Chicago-Police-Department-Illinois-Uniform-Crime-R/c7ck-438e/\n",
    "# below: retrieve first 1000 rows\n",
    "response = requests.get('https://data.cityofchicago.org/resource/c7ck-438e.csv')\n",
    "assert response.status_code == 200\n",
    "data = response.content.decode('utf-8')\n",
    "df = pd.read_csv(StringIO(data))\n",
    "df.to_csv(crime_data_path + \"Chicago_Police_Department_-_Illinois_Uniform_Crime_Reporting__IUCR__Codes.csv\") \n",
    "crime_codes_csv_path = crime_data_path + \"Chicago_Police_Department_-_Illinois_Uniform_Crime_Reporting__IUCR__Codes.csv\"\n",
    "\n",
    "########################### POPULATION DATA ########################################################################################################\n",
    "population_data_path = \"./data/population_data/\"\n",
    "# source 1: https://data.cityofchicago.org/Facilities-Geographic-Boundaries/Population-by-2010-Census-Block/5yjb-v3mj/about_data \n",
    "population_census_csv_path = population_data_path + \"Population_by_2010_Census_Block.csv\"\n",
    "# source 2: https://data.cityofchicago.org/Facilities-Geographic-Boundaries/Boundaries-Census-Tracts-2010/5jrd-6zik \n",
    "census_boundaries_csv_path = population_data_path + \"CensusTractsTIGER2010.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4916f89-14b7-4ec8-9459-210370374c78",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 1. Load and Process Data\n",
    "<a id='load-and-process-data'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7f58b9-4db5-454d-a893-3ee5791022ae",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1.1. Airbnb data\n",
    "<a id='airbnb-data'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ae2754c-043b-406b-a90c-3b8544d6a839",
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open(listings_csv_path, 'rt', encoding='utf-8') as f:\n",
    "    file_content = f.read()\n",
    "\n",
    "df_listings = pd.read_csv(listings_csv_path, compression='gzip', header=0, sep=',', quotechar='\"')\n",
    "\n",
    "# df_listings = pd.read_csv(listings_csv_path)\n",
    "# df_listings.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8679c27-ef01-42f3-89b5-11654b4b05b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'listing_url', 'scrape_id', 'last_scraped', 'source', 'name',\n",
       "       'description', 'neighborhood_overview', 'picture_url', 'host_id',\n",
       "       'host_url', 'host_name', 'host_since', 'host_location', 'host_about',\n",
       "       'host_response_time', 'host_response_rate', 'host_acceptance_rate',\n",
       "       'host_is_superhost', 'host_thumbnail_url', 'host_picture_url',\n",
       "       'host_neighbourhood', 'host_listings_count',\n",
       "       'host_total_listings_count', 'host_verifications',\n",
       "       'host_has_profile_pic', 'host_identity_verified', 'neighbourhood',\n",
       "       'neighbourhood_cleansed', 'neighbourhood_group_cleansed', 'latitude',\n",
       "       'longitude', 'property_type', 'room_type', 'accommodates', 'bathrooms',\n",
       "       'bathrooms_text', 'bedrooms', 'beds', 'amenities', 'price',\n",
       "       'minimum_nights', 'maximum_nights', 'minimum_minimum_nights',\n",
       "       'maximum_minimum_nights', 'minimum_maximum_nights',\n",
       "       'maximum_maximum_nights', 'minimum_nights_avg_ntm',\n",
       "       'maximum_nights_avg_ntm', 'calendar_updated', 'has_availability',\n",
       "       'availability_30', 'availability_60', 'availability_90',\n",
       "       'availability_365', 'calendar_last_scraped', 'number_of_reviews',\n",
       "       'number_of_reviews_ltm', 'number_of_reviews_l30d', 'first_review',\n",
       "       'last_review', 'review_scores_rating', 'review_scores_accuracy',\n",
       "       'review_scores_cleanliness', 'review_scores_checkin',\n",
       "       'review_scores_communication', 'review_scores_location',\n",
       "       'review_scores_value', 'license', 'instant_bookable',\n",
       "       'calculated_host_listings_count',\n",
       "       'calculated_host_listings_count_entire_homes',\n",
       "       'calculated_host_listings_count_private_rooms',\n",
       "       'calculated_host_listings_count_shared_rooms', 'reviews_per_month',\n",
       "       'geometry'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get rid of the dollar signs\n",
    "df_listings = df_listings.dropna(subset=['price'])  # remove missings\n",
    "df_listings['price'] = df_listings['price'].astype(str)\n",
    "df_listings['price'] = df_listings['price'].str.replace('$', '').str.replace(',', '').astype(float)\n",
    "\n",
    "# listings data as gdf\n",
    "gdf_listings = gpd.GeoDataFrame(df_listings, geometry=gpd.points_from_xy(df_listings.longitude, df_listings.latitude))\n",
    "gdf_listings.crs = 'EPSG:4326'\n",
    "gdf_listings.to_crs(epsg=26916, inplace=True)  # common projection\n",
    "\n",
    "# remove one outlier value (faulty data?)\n",
    "gdf_listings = gdf_listings[gdf_listings['price'] <= 10000]\n",
    "\n",
    "# only keep certain columns for now\n",
    "# gdf_listings_filtered = gdf_listings[['geometry', 'accommodates', 'price', 'review_scores_location', 'review_scores_rating', 'reviews_per_month']].copy()\n",
    "gdf_listings.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "651729dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3998331125199975\n",
      "0.4389075733734058\n",
      "0.49516266305926887\n",
      "-0.06226923929452508\n",
      "0.06397931465624156\n",
      "0.05069734174196325\n",
      "0.09205110004297445\n"
     ]
    }
   ],
   "source": [
    "#calculate correlations\n",
    "print(gdf_listings['price'].corr(gdf_listings['beds']))\n",
    "print(gdf_listings['price'].corr(gdf_listings['bedrooms']))\n",
    "print(gdf_listings['price'].corr(gdf_listings['accommodates']))\n",
    "print(gdf_listings['price'].corr(gdf_listings['number_of_reviews']))\n",
    "print(gdf_listings['price'].corr(gdf_listings['review_scores_rating']))\n",
    "print(gdf_listings['price'].corr(gdf_listings['review_scores_accuracy']))\n",
    "print(gdf_listings['price'].corr(gdf_listings['review_scores_cleanliness']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b9da6f-4d90-4ac1-8ee4-ea8b9ad78715",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1.2. Crime data\n",
    "<a id='crime-data'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6812ccf-7f6b-4363-9db7-2ece239c421a",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/crime_data/Crimes_-_2001_to_Present.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df_crime \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcrime_csv_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mlen\u001b[39m(df_crime\u001b[38;5;241m.\u001b[39mindex)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#It has 8 million rows! Need to reduce this for now to have quicker processing/visualizations.\u001b[39;00m\n",
      "File \u001b[1;32m~\\Desktop\\LOCAL_FILES\\1_TU_REPOS_ASSIGNMENTS\\DOPP\\Airbnb\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    945\u001b[0m )\n\u001b[0;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Desktop\\LOCAL_FILES\\1_TU_REPOS_ASSIGNMENTS\\DOPP\\Airbnb\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    608\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\Desktop\\LOCAL_FILES\\1_TU_REPOS_ASSIGNMENTS\\DOPP\\Airbnb\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Desktop\\LOCAL_FILES\\1_TU_REPOS_ASSIGNMENTS\\DOPP\\Airbnb\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1703\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1704\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1705\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1706\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1707\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1708\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1709\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1710\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1711\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1712\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1713\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1714\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1715\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1716\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\Desktop\\LOCAL_FILES\\1_TU_REPOS_ASSIGNMENTS\\DOPP\\Airbnb\\venv\\Lib\\site-packages\\pandas\\io\\common.py:863\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    859\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    860\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    861\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    862\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 863\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    864\u001b[0m             handle,\n\u001b[0;32m    865\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    866\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    867\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    868\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    869\u001b[0m         )\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    871\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    872\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/crime_data/Crimes_-_2001_to_Present.csv'"
     ]
    }
   ],
   "source": [
    "df_crime = pd.read_csv(crime_csv_path)\n",
    "len(df_crime.index)\n",
    "#It has 8 million rows! Need to reduce this for now to have quicker processing/visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7328870-6640-4e4e-bf8c-abd4d62f8974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename columns for consistency\n",
    "df_crime.rename(columns={'Latitude': 'latitude', 'Longitude': 'longitude'}, inplace=True)\n",
    "df_crime.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a077db5a-fb40-4779-b79c-083c615f7577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# types of crime\n",
    "print(len(df_crime[\"Primary Type\"].unique()))\n",
    "df_crime[\"Primary Type\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd51a4e3-9d2e-4414-a3fd-79c501f9671c",
   "metadata": {},
   "source": [
    "### 1.2.1 Filter\n",
    "<a id='filter'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ad0614-8b71-43d4-860f-d22c7db1f3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_crime['Date'] = pd.to_datetime(df_crime['Date'], format='%m/%d/%Y %I:%M:%S %p')\n",
    "df_crime_recent = df_crime[df_crime['Date'].dt.year >= 2018] # only take last n years for now\n",
    "\n",
    "crimes_to_inspect = [\"HOMICIDE\"]  # a list of crime types from the \"Primary Type\" column\n",
    "df_crime_recent = df_crime_recent[df_crime_recent[\"Primary Type\"].isin(crimes_to_inspect)]\n",
    "df_crime_recent = df_crime_recent[~df_crime_recent['Domestic']]\n",
    "\n",
    "# randomly sample a fraction for reduced size\n",
    "df_crime_recent_sampled = df_crime_recent # .sample(frac=0.30, random_state=1)\n",
    "\n",
    "# remove strange location outliers\n",
    "df_crime_recent_sampled = df_crime_recent_sampled[\n",
    "    (df_crime_recent_sampled['latitude'] >= 41) & \n",
    "    (df_crime_recent_sampled['latitude'] <= 42.5) & \n",
    "    (df_crime_recent_sampled['longitude'] >= -88.5) &      \n",
    "    (df_crime_recent_sampled['longitude'] <= -87)]\n",
    "\n",
    "gdf_crime = gpd.GeoDataFrame(df_crime_recent_sampled, geometry=[Point(xy) for xy in zip(df_crime_recent_sampled.longitude, df_crime_recent_sampled.latitude)])\n",
    "\n",
    "# change projection\n",
    "gdf_crime.set_crs(epsg=4326, inplace=True)  # set the original CRS to EPSG:4326\n",
    "gdf_crime = gdf_crime.to_crs(epsg=26916)  # reproject to EPSG:26916\n",
    "\n",
    "gdf_crime_filtered = gdf_crime[['ID', 'Date', 'IUCR', 'Primary Type', \n",
    "                                      'Description', 'Location Description', 'geometry']].copy()\n",
    "\n",
    "gdf_crime_filtered = gdf_crime_filtered[gdf_crime_filtered['Description'] == 'FIRST DEGREE MURDER']\n",
    "gdf_crime_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0e4fac-4f66-4457-ab08-c8b5671e7328",
   "metadata": {},
   "outputs": [],
   "source": [
    "value_counts = gdf_crime_filtered['Location Description'].value_counts()\n",
    "print(value_counts.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aeaa7ce-c35b-4a52-87a9-51395c73bed1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1.3. Population data\n",
    "<a id='population-data'></a>\n",
    "\n",
    "Merge two data sets to get areas (districts) as well as their population."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4455c6a7-f5f9-4803-ac81-19e2332c3909",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_population_census = pd.read_csv(population_census_csv_path)\n",
    "df_population_census"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f17c7f-16c0-4133-9c7a-f04fe583b0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_population_census[\"TOTAL POPULATION\"].sum() # corresponds to Chicago population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5408058f-c4fe-466b-988a-034b964bcf2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_population_census['CENSUS BLOCK FULL'] = df_population_census['CENSUS BLOCK FULL'].astype(str)\n",
    "df_population_census['GEOID10_Subset'] = df_population_census['CENSUS BLOCK FULL'].str[:11]\n",
    "\n",
    "df_census_grouped = df_population_census.groupby('GEOID10_Subset')['TOTAL POPULATION'].sum().reset_index()\n",
    "df_census_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59c31e9-8a0e-4f38-89b7-9e1f863d6f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_census_boundaries = pd.read_csv(census_boundaries_csv_path)\n",
    "df_census_boundaries['the_geom'] = df_census_boundaries['the_geom'].apply(wkt.loads)\n",
    "gdf_census_boundaries = gpd.GeoDataFrame(df_census_boundaries, geometry='the_geom')\n",
    "gdf_census_boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a697ff-c805-4a08-9184-76ff262fbf8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# join the data\n",
    "gdf_census_boundaries['GEOID10'] = gdf_census_boundaries['GEOID10'].astype(str)\n",
    "gdf_population_merged = gdf_census_boundaries.merge(df_census_grouped, left_on='GEOID10', right_on='GEOID10_Subset', how='left')\n",
    "\n",
    "# we only need a few columns\n",
    "selected_columns = ['the_geom', 'GEOID10', 'TOTAL POPULATION']  \n",
    "gdf_population_merged = gdf_population_merged[selected_columns]\n",
    "\n",
    "# obtain polygon area\n",
    "gdf_population_merged.crs = 'EPSG:4326'\n",
    "gdf_population_merged.to_crs(epsg=26916, inplace=True)\n",
    "gdf_population_merged['Area_km2'] = gdf_population_merged['the_geom'].area / 1e6\n",
    "\n",
    "# population density\n",
    "gdf_population_merged['Population_Density'] = gdf_population_merged['TOTAL POPULATION'] / gdf_population_merged['Area_km2']\n",
    "gdf_population_merged # FINAL POPULATION GDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc711edd-ed11-49ae-97d7-2d8129dd7647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# validity checks\n",
    "print(\"Total Area of Chicago\", gdf_population_merged[\"Area_km2\"].sum())\n",
    "print(\"Total Population of Chicago\", gdf_population_merged[\"TOTAL POPULATION\"].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6d5458-7993-4840-be3a-afe162e466ef",
   "metadata": {},
   "source": [
    "--> the whole population got matched to a polygon (it matches the sum from before). Great!\n",
    "\n",
    "The numbers for population and area roughly add up to the correct amount (https://en.wikipedia.org/wiki/Chicago)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a2c10a-a81b-40af-ba48-7ac44273c80e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 1.4. Additional City data\n",
    "<a id='additional-city-data'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abab719b-4259-41d6-ae0d-481fc74b2500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parks\n",
    "# https://data.cityofchicago.org/Parks-Recreation/Parks-Shapefiles-deprecated-November-2016-/5msb-wbxn/about_data\n",
    "gdf_parks = gpd.read_file(population_data_path + 'Parks_Aug2012.shp')\n",
    "gdf_parks.crs = 'EPSG:3435'\n",
    "gdf_parks.to_crs(epsg=26916, inplace=True)  # common projection\n",
    "gdf_parks.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764c22a1-c46e-4fe7-9585-8ad2e0671f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boulevards\n",
    "# https://data.cityofchicago.org/Environment-Sustainable-Development/Open-Spaces-Boulevards-KML/uhyd-nthd/about_data --> select shp (not kml) file from this site\n",
    "gdf_boulevards = gpd.read_file(population_data_path + 'DATA_ADMIN_OPNSP_BOULEVARDS.shp')\n",
    "gdf_boulevards.crs = 'EPSG:3435'\n",
    "gdf_boulevards.to_crs(epsg=26916, inplace=True)  # common projection\n",
    "gdf_boulevards.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c916e526-d707-4381-9c5c-ddbbf78dc342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Riverwalks\n",
    "# https://data.cityofchicago.org/Environment-Sustainable-Development/Open-Spaces-Riverwalk-KML/22bv-uv6r/about_data --> select shp (not kml) file from this site \n",
    "gdf_riverwalks = gpd.read_file(population_data_path + 'DATA_ADMIN_OPNSP_RIVERWALK.shp')\n",
    "gdf_riverwalks.crs = 'EPSG:3435'\n",
    "gdf_riverwalks.to_crs(epsg=26916, inplace=True)  # common projection\n",
    "gdf_riverwalks.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d42fc47-bb7f-4296-8c65-5cd15c4af30b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 2. Exploratory Data Analysis (EDA)\n",
    "<a id='exploratory-data-analysis'></a>\n",
    "\n",
    "Now we have 3 main dataframes:\n",
    "* `gdf_listings` (Airbnb data)\n",
    "* `gdf_crime_filtered` (Crime data)\n",
    "* `gdf_population_merged` (Population/geographic data)\n",
    "\n",
    "Moreover, 3 \"side\" dataframes: `gdf_parks`, `gdf_boulevards`, `gdf_riverwalks`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd54c5db-edc5-48e0-a37f-d72af072f3e0",
   "metadata": {},
   "source": [
    "## 2.1. Geographic EDA\n",
    "<a id='geographic-eda'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8df21f3-a7a4-4ade-9c1b-0c2cc0c6492e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "gdf_population_merged.plot(column='TOTAL POPULATION', ax=ax, legend=True,\n",
    "                          legend_kwds={'label': \"Number of residents by area\",\n",
    "                                       'orientation': \"horizontal\"})\n",
    "plt.title('Total Population by Area')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebec0bf-2fd5-46c0-a846-5b1ad3d9b3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "gdf_population_merged.plot(column='Population_Density', ax=ax, legend=True,\n",
    "                          vmin=0, vmax=14000,  # adjust scale\n",
    "                          legend_kwds={'label': \"Population Density\",\n",
    "                                       'orientation': \"horizontal\"})\n",
    "plt.title('Population Density per Area')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565b6453-091f-4d5a-b4d4-0f95802274e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot all three\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "gdf_parks.plot(ax=ax, color='green', edgecolor='green', alpha=0.5)\n",
    "gdf_boulevards.plot(ax=ax, color='red', edgecolor='red', alpha=0.5)\n",
    "gdf_riverwalks.plot(ax=ax, color='blue', edgecolor='blue', alpha=0.5)\n",
    "ax.set_xlabel('Longitude')\n",
    "ax.set_ylabel('Latitude')\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "ax.set_aspect('equal')\n",
    "plt.title('Parks, Boulevards and Riverwalks in Chicago')\n",
    "# plt.legend(['Parks', 'Boulevards', 'Riverwalks'])\n",
    "park_patch = mpatches.Patch(color='green', label='Parks')\n",
    "boulevard_patch = mpatches.Patch(color='red', label='Boulevards')\n",
    "riverwalk_patch = mpatches.Patch(color='blue', label='Riverwalks')\n",
    "plt.legend(handles=[park_patch, boulevard_patch, riverwalk_patch])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706194e8-6578-4620-92b7-96133f4ffc60",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "gdf_population_merged.plot(ax=ax, color='white', edgecolor='black', alpha=0.5)\n",
    "gdf_listings.plot(ax=ax, color='blue', edgecolor='white', alpha=0.3)\n",
    "gdf_crime_filtered.plot(ax=ax, color='red', edgecolor='white', alpha=0.2)\n",
    "ax.set_xlabel('Longitude')\n",
    "ax.set_ylabel('Latitude')\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "ax.set_aspect('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8ce9ad-5491-4503-9ad3-b3e99fed0d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_listings['geohash'] = gdf_listings.apply(lambda row: geohash.encode(row['latitude'], row['longitude'], precision=5), axis=1)\n",
    "\n",
    "\n",
    "geometry = [Point(xy) for xy in zip(gdf_listings['longitude'], gdf_listings['latitude'])]\n",
    "geo_df = gpd.GeoDataFrame(gdf_listings, geometry=geometry)\n",
    "\n",
    "# Define the boundaries of the  grid\n",
    "num_of_columns, num_of_rows = 100, 100\n",
    "minx, miny, maxx, maxy = geo_df.geometry.total_bounds\n",
    "dx = (maxx - minx) / num_of_columns  # width of grid cell\n",
    "dy = (maxy - miny) / num_of_rows  # height of grid cell\n",
    "\n",
    "\n",
    "grid = []\n",
    "for x in range(num_of_columns):\n",
    "    for y in range(num_of_rows):\n",
    "        xmin = minx + x * dx\n",
    "        xmax = minx + (x+1) * dx\n",
    "        ymin = miny + y * dy\n",
    "        ymax = miny + (y+1) * dy\n",
    "        grid.append(Polygon([(xmin,ymin), (xmax,ymin), (xmax,ymax), (xmin,ymax)]))\n",
    "grid = gpd.GeoDataFrame(grid, columns=['geometry'], crs=geo_df.crs)\n",
    "\n",
    "\n",
    "joined = gpd.sjoin(geo_df, grid, how='left', op='within')\n",
    "joined.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab029d52-6adc-4986-b686-dd47fafc8981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 1\n",
    "fig, ax = plt.subplots(1, 1, figsize=(14, 12))\n",
    "grid.plot(ax=ax, facecolor='none', edgecolor='grey')  # plot grid\n",
    "joined.plot(ax=ax, markersize=10, color='darkred', alpha=0.4)  # plot the scatters \n",
    "plt.title('Chicago AirBnBs grouped by $100\\\\times100$ grids')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7bdbb7-2c99-406f-8fb6-1497c606a54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 2\n",
    "joined.groupby(joined.index_right)['price'].mean()\n",
    "average_price = joined.groupby(joined.index_right)['price'].mean()\n",
    "\n",
    "# join the average prices back to the grid\n",
    "grid['price'] = average_price\n",
    "\n",
    "# plot the grid with colors according to the average price\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "vmin = 100  \n",
    "vmax = 1000  \n",
    "grid.plot(column='price', ax=ax, legend=True, vmin=vmin, vmax=vmax)\n",
    "plt.title('Average price per grid cell')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2798cb-4928-4dea-82fb-a688de6fa3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 3\n",
    "fig, ax = plt.subplots(1, 1, figsize=(14, 12))\n",
    "\n",
    "# create a color map\n",
    "cmap = plt.get_cmap('viridis')\n",
    "colors = cmap(joined['neighbourhood_cleansed'].astype('category').cat.codes)\n",
    "\n",
    "# create a size variable that is proportional to the price_float value\n",
    "sizes = joined['price'] / joined['price'].max() * 1200\n",
    "\n",
    "# plot the scatters \n",
    "joined.plot(ax=ax, markersize=sizes, color=colors, alpha=.17)\n",
    "plt.title('AirBnB prices in Chicago')\n",
    "plt.suptitle('The bigger the circle, the larger the price', fontsize=10, y=0.92)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f89098-7d46-4f70-aaac-6610c2dd27ef",
   "metadata": {},
   "source": [
    "## 2.2. Non-Geographic EDA\n",
    "<a id='non-geographic-eda'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3812e577-4165-44ce-bf45-71b7462d2257",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = df_listings[\"accommodates\"] # gdf_population_merged['Population_Density']\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(metric, bins=30, color='blue', edgecolor='black')\n",
    "plt.title('Distributions of number of people an AirBnB can accommodate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8fb389-e560-4265-9129-3b7d96330650",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_listings.groupby('neighbourhood_cleansed')['price'].mean().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47a885f-fbb9-4fbe-8885-4e0697ba8330",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_listings[['neighbourhood_cleansed', 'price', 'bedrooms', 'beds', 'accommodates']].groupby('neighbourhood_cleansed').median().sort_values(by='price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2408bb79-5ab3-4f25-a345-9569356023e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10_nbds = gdf_listings['neighbourhood_cleansed'].value_counts()[:10].index\n",
    "top_10_nbds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58c5714-44f7-4011-9d69-a773dc1da4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_top10 = gdf_listings.loc[gdf_listings['neighbourhood_cleansed'].isin(top_10_nbds)]\n",
    "df_top10.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd14c8f1-ddb7-4c31-b885-b57757c0717e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(28,10))\n",
    "sns.violinplot(data=df_top10, x='neighbourhood_cleansed', y='price')\n",
    "plt.rcParams['font.size'] = 14\n",
    "plt.xticks(rotation=45)\n",
    "plt.xlabel('Neighborhood')\n",
    "plt.ylabel('Price per night (USD)')\n",
    "plt.title('Price distribution by neighborhood')\n",
    "plt.ylim(0, 400)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9a3867-680d-46de-83e9-6713537302e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_listings[['neighbourhood_cleansed', 'price', 'bedrooms', 'beds', 'accommodates']].groupby('neighbourhood_cleansed').median().sort_values(by='price')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c55f9a1-b39c-4485-abfd-21689766a615",
   "metadata": {},
   "source": [
    "# 3. In-Depth Analysis & Modeling - Airbnb Locations\n",
    "<a id='in-depth-analysis-airbnb-locations'></a>\n",
    "Here, we create a final dataset which is based on \"population\". When computing further statistics for each district, it's important to account for the district's size (\"normalization\"). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b17ae8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# listings data as gdf\n",
    "gdf_listings = gpd.GeoDataFrame(df_listings, geometry=gpd.points_from_xy(df_listings.longitude, df_listings.latitude))\n",
    "gdf_listings.crs = 'EPSG:4326'\n",
    "gdf_listings.to_crs(epsg=26916, inplace=True)  # common projection\n",
    "\n",
    "# remove one outlier value (faulty data?)\n",
    "gdf_listings = gdf_listings[gdf_listings['price'] <= 10000]\n",
    "\n",
    "# only keep certain columns for now\n",
    "gdf_listings_filtered = gdf_listings[['geometry', 'accommodates', 'price', 'review_scores_location', \n",
    "                                      'review_scores_rating', 'reviews_per_month']].copy()\n",
    "gdf_listings_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b682ec8d-66cc-4bde-9cf2-565ea47c49fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# join Airbnb and population data\n",
    "gdf_joined = gpd.sjoin(gdf_listings_filtered, gdf_population_merged, how=\"inner\", op=\"within\")\n",
    "statistics = gdf_joined.groupby('GEOID10').agg(  # compute various descriptive statistics\n",
    "    Airbnb_Count=('price', 'count'),\n",
    "    Median_Price=('price', 'median'),\n",
    "    Average_Price=('price', 'mean'),\n",
    "    Median_Review_Score=('review_scores_rating', 'median'))\n",
    "statistics = statistics.join(gdf_population_merged.set_index('GEOID10')['Area_km2'])\n",
    "\n",
    "# calculate Airbnbs per kmÂ²\n",
    "statistics['Airbnb_per_km2'] = statistics['Airbnb_Count'] / statistics['Area_km2']\n",
    "\n",
    "# join back\n",
    "gdf_population_merged_stats = gdf_population_merged.merge(statistics, on='GEOID10', how='left')\n",
    "gdf_population_merged_stats.drop(columns=['Area_km2_y'], inplace=True)\n",
    "gdf_population_merged_stats.rename(columns={'Area_km2_x': 'Area_km2'}, inplace=True)\n",
    "\n",
    "# add crime counts\n",
    "gdf_joined_crime = gpd.sjoin(gdf_crime_filtered, gdf_population_merged_stats, how=\"inner\", op=\"within\")\n",
    "gdf_crime_counts = gdf_joined_crime.groupby('GEOID10').size().reset_index(name='Crime_Count')\n",
    "\n",
    "# merge back\n",
    "gdf_population_final = gdf_population_merged_stats.merge(gdf_crime_counts, on='GEOID10', how='left')\n",
    "gdf_population_final.rename(columns={'Crime_Count_x': 'Crime_Count'}, inplace=True)\n",
    "gdf_population_final['Crime_Count'].fillna(0, inplace=True)\n",
    "gdf_population_final['Crimes_per_km2'] = gdf_population_final['Crime_Count'] / gdf_population_final['Area_km2']  # crime density column\n",
    "gdf_population_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35144197-a60c-4dd7-8f4a-48b92e823c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_population_final_nonan = gdf_population_final.dropna(subset=['Median_Price', 'Crime_Count'])  # drop some missings\n",
    "\n",
    "X = gdf_population_final_nonan[['Crimes_per_km2']]\n",
    "y = gdf_population_final_nonan['Airbnb_per_km2']\n",
    "X = sm.add_constant(X)  # add a constant to the model (the intercept)\n",
    "\n",
    "model = sm.OLS(y, X).fit()\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5eb2ab-a695-4e31-9ec7-1dae54317239",
   "metadata": {},
   "source": [
    "# 4. In-Depth Analysis & Modeling - Airbnb Prices\n",
    "<a id='in-depth-analysis-airbnb-prices'></a>\n",
    "\n",
    "In a first step, we will extend the gdf_listings by some further data. We add 3 columns:\n",
    "\n",
    "* `crime_count` which tells us how many homicides occured in a 2km radius\n",
    "* `distance_to_center` which gives the distance to the city center\n",
    "* `has_park_within_100m` which gives a boolean indicator whether there is a park nearby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3190a257-4689-4f39-bd6b-fa7ad2b7610d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'buffer' not in gdf_listings.columns:\n",
    "    gdf_listings['buffer'] = gdf_listings['geometry'].buffer(2000)  # 2000 meters = 2km\n",
    "\n",
    "if 'gdf_listings_buffer' not in locals():\n",
    "    gdf_listings_buffer = gdf_listings[['id', 'buffer']].copy()\n",
    "    gdf_listings_buffer.set_geometry('buffer', inplace=True)\n",
    "\n",
    "# Perform the spatial join\n",
    "joined = gpd.sjoin(gdf_listings_buffer, gdf_crime_filtered, how='left', op='intersects')\n",
    "\n",
    "# Count the crimes and merge with the original dataframe\n",
    "crime_count = joined.groupby('id').size().reset_index(name='crime_count_temp')\n",
    "gdf_listings = gdf_listings.merge(crime_count, on='id', how='left')\n",
    "\n",
    "# Update crime_count column\n",
    "gdf_listings['crime_count'] = gdf_listings['crime_count_temp'].fillna(0)\n",
    "gdf_listings.drop(['buffer', 'crime_count_temp'], axis=1, inplace=True, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab09f93-0c25-493a-943d-a66717f3f4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_listings[[\"id\", \"crime_count\"]].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1542bf8d-316d-4881-b824-c2de080fe73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get distance to city center for each airbnb\n",
    "\n",
    "# Create a Point object for the given coordinates\n",
    "center_point = Point(448156, 4635454)\n",
    "\n",
    "# Calculate distances and add/update the column in gdf_listings\n",
    "if 'distance_to_center' in gdf_listings.columns:\n",
    "    # Update the column if it already exists\n",
    "    gdf_listings['distance_to_center'] = gdf_listings['geometry'].distance(center_point)\n",
    "else:\n",
    "    # Create the column if it does not exist\n",
    "    gdf_listings = gdf_listings.assign(distance_to_center=gdf_listings['geometry'].distance(center_point))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4cc633-eaa4-4966-a75e-97a74c5da27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_price_relationships(x:str, facet:str, ax_scaling:str='free', outlier_cutoff:float=6000):\n",
    "    g =(ggplot(gdf_listings[gdf_listings['price'] <= outlier_cutoff], # filter out the outlier\n",
    "            aes(x=x, y='price', color=facet)) +\n",
    "            geom_point(color='black', alpha=0.2, size=2, stroke=0.4) +\n",
    "            geom_smooth(method='lm', color='r', se=False) +\n",
    "            facet_wrap(facets=facet, scales=ax_scaling) \n",
    "    )\n",
    "    return g\n",
    "\n",
    "plot_price_relationships(x='accommodates', facet='room_type', ax_scaling='fixed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3599084-eeda-445e-a8ea-cd0bb840ca55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_price_relationships_r2(x:str, facet:str, df:pd.DataFrame=gdf_listings, ax_scaling:str='free', outlier_cutoff:float=6000):\n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    df = df[df['price'] <= outlier_cutoff]\n",
    "    \n",
    "    # calculate R^2 for each facet\n",
    "    facets = df[facet].unique()\n",
    "    r2_values = {}\n",
    "    for f in facets:\n",
    "        model = smf.ols(formula=f'price ~ {x}', data=df[df[facet] == f]).fit()\n",
    "        # r2_values[f] = f\"R2 = {round(model.rsquared, 4)}\"\n",
    "        # r2_values[f] = f\"$R^2$ = {round(model.rsquared, 4)}\\n$\\\\beta$ = {round(model.params[x],2)}\"\n",
    "        r2_values[f] = f\"$R^2$ = {round(model.rsquared, 4)}\\n$\\\\beta$ = {round(model.params[x],2)}\\n$p$ = {round(model.pvalues[x],4)}\"\n",
    "    \n",
    "    # create a new dataframe for R^2 values\n",
    "    r2_df = pd.DataFrame(list(r2_values.items()), columns=[facet, 'R2'])\n",
    "    g = (ggplot(df, aes(x=x, y='price', color=facet)) +\n",
    "         geom_point(color='black', alpha=0.2, size=2, stroke=0.4) +\n",
    "         geom_smooth(method='lm', color='r', se=False) +\n",
    "         geom_text(data=r2_df, mapping=aes(label='R2'), color='b', x=len(df[x].unique())/4, y=outlier_cutoff-1700) +\n",
    "         facet_wrap(facets=facet, scales=ax_scaling))\n",
    "    return g\n",
    "\n",
    "plot_price_relationships_r2(x='accommodates', facet='room_type', ax_scaling='fixed')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17fc58d-9a85-4e81-ad32-7dee692e8bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "facet = 'host_response_time'\n",
    "plot_price_relationships(x='accommodates', facet=facet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2930fe4b-7448-48e1-abcb-f9f8b3d93579",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_price_relationships_r2(x='beds', facet='room_type', ax_scaling='fixed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c950e3-da04-449d-be64-e035b1a71f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_price_relationships_r2(x='accommodates', facet='property_type', ax_scaling='fixed') + theme(aspect_ratio=1, figure_size=(25, 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ade29a4-2f04-4229-8838-2bfc8dc805ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "facet = 'room_type'\n",
    "plot_price_relationships_r2(x='review_scores_cleanliness', facet=facet, ax_scaling='fixed')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90335064-5b59-48c9-97bc-1816fd59524f",
   "metadata": {},
   "source": [
    "## 4.1 K-means clustering\n",
    "<a id='k-means-clustering'></a>\n",
    "-  ```StandardScaler```: subtracting mean, dividing by stdev $\\to$ $x_{scaled}=\\frac{x - \\bar{x}}{\\sigma}$  \n",
    "-  ```MinMaxScaler```: scaling to squish values btw 0 and 1 $\\to$ $x_{scaled}=(\\frac{x - x_{min}}{x_{max}- x_{min}}) \\times (x_{max}- x_{min}) - x_{min}$  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d8ce30-d335-4570-8e37-23f497789f39",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 4.1.1 Data Processing steps\n",
    "<a id='data-processing-steps'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d41553-9387-40e0-b8c1-e6e2915106a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "property_type = gdf_listings['property_type'].tolist()\n",
    "pt_dict = {}\n",
    "for pt in property_type:\n",
    "    if pt in pt_dict:\n",
    "        pt_dict[pt] += 1\n",
    "    else:\n",
    "        pt_dict[pt] = 1\n",
    "sorted(pt_dict.items(), key=lambda item: item[1], reverse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0b43a8-393c-4827-88a1-8cf844e996b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_object_to_numeric(dataframe:pd.DataFrame):\n",
    "    df = dataframe.copy()\n",
    "    for col in df.columns:\n",
    "        try:\n",
    "            df[col] = df[col].astype('int64')\n",
    "        except TypeError:\n",
    "            try:\n",
    "                df[col] = df[col].astype('float64')\n",
    "            except TypeError:\n",
    "                pass\n",
    "        except ValueError:\n",
    "            try:\n",
    "                df[col] = df[col].astype('float64')\n",
    "            except ValueError:\n",
    "                pass\n",
    "    return df\n",
    "\n",
    "numerical_df = convert_object_to_numeric(gdf_listings)\n",
    "numerical_df = numerical_df.select_dtypes(include=['int64', 'float64'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eaca779-5f73-498e-affc-4fc12661573c",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_df2 = numerical_df.copy()\n",
    "numerical_df2.drop(['neighbourhood_group_cleansed','bathrooms','calendar_updated','reviews_per_month',\n",
    "                   'review_scores_rating', 'review_scores_accuracy', 'review_scores_cleanliness', \n",
    "                   'review_scores_checkin', 'review_scores_communication', 'review_scores_location', 'review_scores_value',\n",
    "                    'latitude', 'longitude', 'availability_30'], axis=1, inplace=True)\n",
    "numerical_df2['bedrooms'].fillna(1, inplace=True)\n",
    "numerical_df2['beds'].fillna(1, inplace=True)\n",
    "print(numerical_df2['bedrooms'].isna().sum())\n",
    "print(numerical_df2['beds'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca2d86f-0786-4f3f-b73f-2bb47000beaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_df3 = numerical_df2[['accommodates', 'bedrooms', 'beds', 'availability_60', 'availability_90', 'availability_365', 'number_of_reviews', 'price']]\n",
    "numerical_df3.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ff8da9-c64c-4352-a1b0-9c8b5d828f96",
   "metadata": {},
   "source": [
    "### 4.1.2 The Model\n",
    "<a id='the-model'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aad9354-b65f-4f9a-bf14-072ee20349fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "stds = StandardScaler()\n",
    "mmxs = MinMaxScaler()\n",
    "\n",
    "df_std = pd.DataFrame(stds.fit_transform(numerical_df3))\n",
    "df_mmx = pd.DataFrame(mmxs.fit_transform(numerical_df3))\n",
    "df_std.columns, df_mmx.columns = numerical_df3.columns, numerical_df3.columns\n",
    "display(df_std.head())\n",
    "display(df_mmx.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a56b92-3240-40fd-82d8-3594b4a7f7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = numerical_df3.values\n",
    "X = df_std.values\n",
    "\n",
    "# kmeans = KMeans(n_clusters=8, init='k-means++', n_init='warn', max_iter=300, tol=0.0001, verbose=1, random_state=None, copy_x=True, algorithm='lloyd')\n",
    "kmeans = KMeans(n_clusters=4, random_state=0)\n",
    "kmeans.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810e652f-4ecc-4d47-a9cd-2e9fdd6e0664",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_df3['cluster'] = kmeans.labels_\n",
    "df_std['cluster'] = kmeans.labels_\n",
    "numerical_df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da13794-a901-485a-a24f-671e963d9d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop overlapping columns from the right dataframe before the merge\n",
    "joined = pd.read_csv(airbnb_data_path + \"listings.csv.gz\")\n",
    "joined = joined.dropna(subset=['price']) # remove missings\n",
    "joined['price'] = joined['price'].astype(str)\n",
    "joined['price'] = joined['price'].str.replace('$', '').str.replace(',', '').astype(float)\n",
    "columns_to_drop = [col for col in joined.columns if col in numerical_df2.columns and col not in ['id', 'price']]\n",
    "\n",
    "joined_drop = joined.drop(columns=columns_to_drop, inplace=False)\n",
    "\n",
    "joined2 = numerical_df2.merge(joined_drop, on=['id', 'price'])\n",
    "print(joined_drop.shape)\n",
    "print(joined2.columns)\n",
    "print(joined2.shape)\n",
    "print(len(kmeans.labels_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0cb5e08-27e4-4d9a-9dcb-b9b8ff9da559",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop overlapping columns from the right dataframe before the merge\n",
    "columns_to_drop = [col for col in joined.columns if col in numerical_df3.columns and col not in ['accommodates', 'bedrooms', 'beds', 'availability_60', \n",
    "                                                                                                 'availability_90', 'availability_365', 'number_of_reviews', 'price']]\n",
    "joined_drop = joined.drop(columns=columns_to_drop, inplace=False)\n",
    "\n",
    "\n",
    "joined3 = numerical_df3.merge(joined_drop, on=['accommodates', 'bedrooms', 'beds', 'availability_60', 'availability_90', 'availability_365', 'number_of_reviews', 'price'])\n",
    "joined3 = pd.concat([joined2, numerical_df3['cluster']], axis=1)\n",
    "joined3.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afc3802-691c-474d-a27d-e6acd0c4df73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# joined2['cluster'] = joined2['cluster'].astype('str')\n",
    "joined3['cluster'] = joined3['cluster'].astype('str')\n",
    "joined3.cluster.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb82a62-103d-44bf-b712-c4c349295bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_color_sequence = [\"blue\", \"red\", 'fuchsia', 'green']\n",
    "\n",
    "# min(merged_df['price_float']); max(merged_df['price_float'])\n",
    "beds = gdf_listings['beds'].fillna(0).tolist()\n",
    "\n",
    "# joined3 = joined3.merge(gdf_listings[['id', 'longitude', 'latitude']], on=['id'])\n",
    "joined3 = pd.concat([joined3, joined[['latitude', 'longitude']]])\n",
    "\n",
    "print(\"Interactive geoplot:\\ndot color -> cluster\\ndot size -> price (higher price, bigger dot)\")\n",
    "fig = px.scatter_mapbox(joined3, lat=\"latitude\", lon=\"longitude\", hover_name=\"beds\",\n",
    "                        hover_data=[\"price\", \"beds\"],\n",
    "                        color=\"cluster\", zoom=10, height=500, size=joined3['beds'].fillna(0.5)*10000, opacity = 0.65, size_max = 15, #size=[80]*merged_df.shape[0],\n",
    "                        # color_discrete_sequence = custom_color_sequence,\n",
    "                        color_continuous_scale=\"Jet\",\n",
    "                        range_color=[100, 700])\n",
    "\n",
    "fig.update_layout(mapbox_style=\"open-street-map\")\n",
    "fig.update_geos(projection_type=\"natural earth\")\n",
    "fig.update_layout(margin={\"r\":0, \"t\":0, \"l\":0, \"b\":0})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d2036f-3725-4c10-a58e-0e7248cd8040",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's see what factors contributed to the clusters\n",
    "cmap='YlGnBu'\n",
    "print('true values:')\n",
    "display(numerical_df3.groupby('cluster').mean().style.background_gradient(cmap=cmap))\n",
    "\n",
    "print('scaled values:')\n",
    "display(df_std.groupby('cluster').mean().style.background_gradient(cmap=cmap))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5ab7f7",
   "metadata": {},
   "source": [
    "## 4.2 Associate Rule Learning (unsupervised co-occurence analysis)\n",
    "<a id='associate-rule-learning'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0d3042",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_listings.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ec9597",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_listings['amenities'] = gdf_listings['amenities'].astype('str')\n",
    "amenities = gdf_listings['amenities']\n",
    "\n",
    "ls_amen = amenities.apply(lambda x: len(x.strip('[]').split(','))).to_list() # strip and split required as amenities col actually contains lists in str format\n",
    "max_len = max(ls_amen)\n",
    "idx = ls_amen.index(max_len)\n",
    "\n",
    "print(f\"the longest list contains {max_len} amenities\")\n",
    "\n",
    "len(gdf_listings['amenities'].iloc[idx].strip('[]').split(',')) == max_len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d90a354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing -> converting amenities column to actual lists\n",
    "gdf_listings['amenities_list'] = gdf_listings['amenities'].apply(lambda x: x.strip('[]').split(','))\n",
    "\n",
    "gdf_listings['amenities_list'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df949889",
   "metadata": {},
   "outputs": [],
   "source": [
    "amenities_df = pd.DataFrame(gdf_listings['amenities_list'].apply(set))\n",
    "\n",
    "\n",
    "amenities_ls = gdf_listings['amenities_list']\n",
    "flattened_list = [elem for sub_ls in amenities_ls for elem in sub_ls]\n",
    "\n",
    "\n",
    "amenities_columns = pd.DataFrame(columns=list(set(sorted(flattened_list))))\n",
    "\n",
    "\n",
    "amenities_df = pd.concat([amenities_df, amenities_columns], axis=1)\n",
    "\n",
    "amenities_df.fillna(0, inplace=True)\n",
    "\n",
    "for idx, row in amenities_df.iterrows(): #omit first column which contains the lists\n",
    "    # get the amenities for the current row\n",
    "    amenities = amenities_ls.loc[idx]\n",
    "    \n",
    "    for amenity in amenities:\n",
    "        # if the amenity is in the DataFrame columns, set its value to 1\n",
    "        if amenity in amenities_df.columns:\n",
    "            amenities_df.loc[idx, amenity] = 1\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53ea7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "amenities_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccf8d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "amenities_dummies = amenities_df.drop(columns=['amenities_list'])\n",
    "\n",
    "amenities_dummies.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a246fee",
   "metadata": {},
   "source": [
    "Metrics used in associate rule mining are the following:\n",
    "\n",
    "- **Support**\n",
    "\n",
    "It indicates how frequently an item set appears in the data set.\n",
    "\n",
    "Let $frq(A)$ be the number of occurrences (frequency) from the total number of transactions $frq(T) = T$:\n",
    "\n",
    "$$\n",
    "supp(A) = \\frac{frq(A)}{T}\n",
    "$$\n",
    "\n",
    "\n",
    "- **Confidence**\n",
    "\n",
    "It says how likely item set B is purchased when item set A is purchased\n",
    "\n",
    "$$\n",
    "conf(A \\to B) = \\frac{supp(A,B)}{supp(A)}\n",
    "$$\n",
    "\n",
    "with $supp(A,B) = \\frac{frq(A,B)}{frq(T)}$ and $supp(A) = \\frac{frq(A)}{frq(T)}$ it holds that:\n",
    "\n",
    "$$\n",
    "conf(A \\to B) = \\frac{supp(A,B)}{supp(A)} = \n",
    "\\frac{\\frac{frq(A,B)}{frq(T)}}{\\frac{frq(A)}{frq(T)}} = \n",
    "\\frac{frq(A,B)}{frq(A)}\n",
    "$$\n",
    "\n",
    "- **Lift**\n",
    "\n",
    "Lift: It says how likely item set B is purchased when item set A is purchased while controlling for how popular item set B is.\n",
    "\n",
    "Lift is the ratio of the observed support to that expected if A and B were independent or equivalently the ratio of the confidence of the rule to the expected confidence of the RHS item set by independence.\n",
    "\n",
    "$$\n",
    "lift(A \\to B) = \\frac{conf(A,B)}{supp(B)} = \\frac{supp(A,B)}{supp(A) \\times supp(B)}\n",
    "$$\n",
    "\n",
    "Note:\n",
    "\n",
    "$$\n",
    "lift(A \\to B) == lift(B \\to A)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7607da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def support(df, cols):\n",
    "  if type(cols) != list:\n",
    "    cols = [cols]\n",
    "    #cols = np.array(cols)\n",
    "  #print(cols)\n",
    "  bool_mask = df[cols].isin([1]).all(axis=1)\n",
    "  frqA = len(df[bool_mask])\n",
    "  T = len(df[cols])\n",
    "  return frqA/T\n",
    "\n",
    "# item set of 3 different products\n",
    "# print(support(retail, [\"Bread\", \"Yogurt\", \"Egg\"]))\n",
    "# # also works with single item sets\n",
    "# support(retail,'Bread')\n",
    "  \n",
    "\n",
    "def confidence(df, A, B):\n",
    "  if type(A)==list and type(B)==list:\n",
    "    AB = A + B\n",
    "  elif type(A)==list and type(B)!=list:\n",
    "    AB = A\n",
    "    AB.append(B)\n",
    "  elif type(A)!=list and type(B)==list:\n",
    "    AB = B\n",
    "    AB.append(A)\n",
    "  elif type(A)!=list and type(B)!=list:\n",
    "    AB = [A, B]\n",
    "  return support(df, AB) / support(df, A)\n",
    "\n",
    "# print(confidence(retail, \"Bread\", \"Egg\"))\n",
    "\n",
    "# confidence(retail, [\"Bread\"], [\"Dog_Food\",\"Flowers\"])\n",
    "\n",
    "\n",
    "def lift(df, A, B):\n",
    "  return confidence(df, A, B) / support(df, B)\n",
    "\n",
    "# lift(retail, \"Bread\", \"Egg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2efa2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate support for each amenity\n",
    "support_values = {amenity: support(amenities_dummies, amenity) for amenity in amenities_dummies.columns}\n",
    "\n",
    "# Filter amenities with support > 10%\n",
    "filter_amenities = [amenity for amenity, support in support_values.items() if support > 0.15]\n",
    "print(\"number of features with support > 15%:\", len(filter_amenities))\n",
    "\n",
    "amenities_names = amenities_dummies[filter_amenities].columns\n",
    "\n",
    "lift_vals = {}\n",
    "\n",
    "for i, a1 in enumerate(amenities_names):\n",
    "    for a2 in amenities_names[i:]:\n",
    "        if a1 != a2:\n",
    "            # print(f\"lift of {a1} and {a2} is {lift(amenities_dummies, a1, a2)}\")\n",
    "            lift_value = lift(amenities_dummies, a1, a2)\n",
    "            combo = f\"{a1} & {a2}\"\n",
    "            lift_vals[combo] = lift_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d72b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "highest_lift = sorted(lift_vals.items(), key=lambda item: item[1], reverse=True)[:10]\n",
    "pd.DataFrame(highest_lift, columns=['Itemsets', 'Lift']).style.background_gradient(cmap=cmap)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe36a6f",
   "metadata": {},
   "source": [
    "\n",
    "- If the lift is equal to 1, it means that the antecedent and the consequent are independent of each other. When two events are independent of each other, no rule can be drawn involving those two events.\n",
    "\n",
    "- If the lift is greater than 1, it means that the antecedent and the consequent are positively correlated. This indicates that the occurrences of the antecedent and the consequent are dependent on one another, and the rule could be useful for predicting the consequent in future data sets.\n",
    "\n",
    "- If the lift is less than 1, it means that the antecedent and the consequent are negatively correlated.\n",
    "\n",
    "So, a lift value of 4.182987 means that the antecedent and the consequent are positively correlated and occur together more than 4 times as often as would be expected if they were independent. This suggests a strong association between the antecedent and the consequent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e81dbc0",
   "metadata": {},
   "source": [
    "further analysis of the amenities column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad1d6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "amenities_dummies['price'] = gdf_listings['price']\n",
    "\n",
    "# removing outlier\n",
    "amenities_dummies = amenities_dummies[amenities_dummies['price'] <= 25000]\n",
    "\n",
    "average_price = {}\n",
    "median_price = {}\n",
    "for col in amenities_dummies.columns:\n",
    "    if col != 'price' and amenities_dummies[col].sum() > 3:\n",
    "        average_price[col] = amenities_dummies[amenities_dummies[col] == 1]['price'].mean()\n",
    "        median_price[col] = amenities_dummies[amenities_dummies[col] == 1]['price'].median()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f39bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "amenities_count = pd.DataFrame(amenities_dummies[amenities_dummies.columns[:-1]].sum().sort_values(ascending=False), columns=['count'])\n",
    "amenities_count.reset_index(inplace=True)\n",
    "amenities_count.rename(columns={'index': 'amenity'}, inplace=True)\n",
    "amenities_count.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce61a93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg = pd.DataFrame(sorted(average_price.items(), key=lambda item: item[1], reverse=True), columns=['amenity', 'average_price'])\n",
    "med = pd.DataFrame(sorted(median_price.items(), key=lambda item: item[1], reverse=True), columns=['amenity', 'median_price'])\n",
    "\n",
    "avg = avg.merge(amenities_count, on='amenity')\n",
    "med = med.merge(amenities_count, on='amenity')\n",
    "\n",
    "\n",
    "combined = avg.merge(med, on=['amenity', 'count'], suffixes=('_avg', '_med'))\n",
    "combined['diff'] = abs(combined['average_price'] - combined['median_price'])\n",
    "combined = combined.sort_values(by='diff', ascending=False)\n",
    "combined.head(20).style.background_gradient(cmap='RdYlGn_r', subset=['average_price', 'median_price', 'count', 'diff'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b751df9e",
   "metadata": {},
   "source": [
    "Features which we also want to consider in predicting the price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe3fea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SELECTING THE AMENITIES WHICH HAVE A DIFFERENCE OF 10% OR LESS BETWEEN THE AVERAGE AND MEDIAN PRICE\n",
    "features_df = combined[combined['diff'] <= ((combined['average_price']+ combined['median_price'])/2) * 0.1] \\\n",
    "            .sort_values(by=['median_price', 'average_price', 'count'], ascending=False) \n",
    "features_df.head(20).style.background_gradient(cmap='RdYlGn_r', subset=['average_price', 'median_price', 'count', 'diff'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412c24c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = features_df['amenity'][:20].tolist()\n",
    "print('the following features will be considered by the feature selector:\\n')\n",
    "for feature in features:\n",
    "    print(feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d411ced-8a4b-48ae-bbba-5e8287dd1ba9",
   "metadata": {},
   "source": [
    "## 4.3 Supervised learning\n",
    "<a id='supervised-learning'></a>\n",
    "\n",
    "Trying to predict Airbnb prices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b458c7a-b589-4afa-8e98-506429c8bf83",
   "metadata": {},
   "source": [
    "### 4.3.1 Data processing\n",
    "<a id='data-processing'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed42db42-77a9-42f6-b038-07045e38e8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gdf_listings['neighbourhood_cleansed'].notna().sum())\n",
    "print(gdf_listings['neighbourhood_cleansed'].isna().sum())\n",
    "gdf_listings.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf592cf-c88e-4f9b-b26c-e25b53393db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_listings[['neighbourhood_cleansed', 'price', 'bedrooms', 'beds', 'accommodates']].groupby('neighbourhood_cleansed').median().sort_values(by='price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403960f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "room_types = pd.get_dummies(gdf_listings['room_type'])\n",
    "room_types = room_types.astype(int)\n",
    "\n",
    "# num_df_for_ml = pd.concat([numerical_df2, room_types, amenities_df[features]], axis=1)\n",
    "num_df_for_ml = pd.concat([numerical_df2, room_types], axis=1)\n",
    "\n",
    "num_df_for_ml.drop(['id', 'host_id', 'scrape_id'], inplace=True, axis=1)\n",
    "num_df_for_ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2f818d",
   "metadata": {},
   "outputs": [],
   "source": [
    "na_col = num_df_for_ml.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194986a0",
   "metadata": {},
   "source": [
    "### 4.3.2 Train-test split\n",
    "<a id='train-test-split'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7345502f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_split(df:pd.DataFrame, valid_size:float):\n",
    "    # Separate features and target variable\n",
    "    X = df.drop(columns='price')\n",
    "    y = df['price']\n",
    "\n",
    "    # Split the data into train and validation sets\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=valid_size)\n",
    "\n",
    "    return X_train, y_train, X_valid, y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e027cb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = create_split(num_df_for_ml, valid_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12235575-818a-4c04-9d6b-58123a2b4228",
   "metadata": {},
   "source": [
    "### 4.3.3 Choosing suitable ML\n",
    "<a id='choosing-suitable-ml'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8fbbba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We choose to train Random Forest Regressor for this task. Then we will compare it with simple Linear regression.\n",
    "\n",
    "ml_methods = []\n",
    "\n",
    "ml_methods.append(RandomForestRegressor)\n",
    "ml_methods.append(LinearRegression)\n",
    "ml_methods.append(XGBRegressor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a513677-7b36-42ec-bd52-1e715cf694fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_price(X_train: pd.DataFrame, y_train: pd.DataFrame, X_test: pd.DataFrame, model_class: type, cv: int = 5) -> tuple:\n",
    "    y_pred = None\n",
    "    trained_model = None\n",
    "    \n",
    "    # Initialize the model\n",
    "    trained_model = model_class()\n",
    "    \n",
    "    # Fit the model on the entire training data\n",
    "    trained_model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = trained_model.predict(X_test)\n",
    "\n",
    "    return y_pred, trained_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d53b2c4",
   "metadata": {},
   "source": [
    "### 4.3.4 Feature Selection\n",
    "<a id='feature-selection'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678f7097",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_split_with_features(df: pd.DataFrame, valid_size: float, num_features: int):\n",
    "    # Separate features and target variable\n",
    "    X = df.drop(columns='price')\n",
    "    y = df['price']\n",
    "\n",
    "    # Feature selection\n",
    "    selector = RFE(RandomForestRegressor(), n_features_to_select=num_features)\n",
    "    selector.fit(X, y)\n",
    "    X_selected = selector.transform(X)\n",
    "    features = [col for col, selected in zip(X.columns, selector.support_) if selected]\n",
    "\n",
    "    print(\"Num Features: %s\" % (selector.n_features_))\n",
    "    print(\"Selected Features: %s\" % (features))\n",
    "\n",
    "    # Split the data into train and validation sets\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X_selected, y, test_size=valid_size)\n",
    "\n",
    "    return X_train, y_train, X_valid, y_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed192be0",
   "metadata": {},
   "source": [
    "### 4.3.5 Scale features\n",
    "<a id='scale-features'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6987e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_features(X_train, X_test):\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    return X_train_scaled, X_test_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17a7408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create split with 10 features\n",
    "X_train_10, y_train, X_test_10, y_test = create_split_with_features(num_df_for_ml, valid_size=0.2, num_features=10)\n",
    "X_train_10_scaled, X_test_10_scaled = scale_features(X_train_10, X_test_10)\n",
    "\n",
    "# Create split with 15 features\n",
    "X_train_15, y_train, X_test_15, y_test = create_split_with_features(num_df_for_ml, valid_size=0.2, num_features=15)\n",
    "X_train_15_scaled, X_test_15_scaled = scale_features(X_train_15, X_test_15)\n",
    "\n",
    "# Create split with 20 features\n",
    "X_train_20, y_train, X_test_20, y_test = create_split_with_features(num_df_for_ml, valid_size=0.2, num_features=20)\n",
    "X_train_20_scaled, X_test_20_scaled = scale_features(X_train_20, X_test_20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce930dec",
   "metadata": {},
   "source": [
    "# 5. Metrics & Evaluation\n",
    "<a id='metrics-evaluation'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c2b625",
   "metadata": {},
   "outputs": [],
   "source": [
    "suitable_metrics = []\n",
    "\n",
    "suitable_metrics.append(sklearn.metrics.mean_squared_error)\n",
    "suitable_metrics.append(sklearn.metrics.mean_absolute_error)\n",
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(sklearn.metrics.mean_squared_error(y_true, y_pred))\n",
    "suitable_metrics.append(rmse)\n",
    "suitable_metrics.append(sklearn.metrics.r2_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a9ffb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_metrics(y_true:pd.DataFrame, y_pred:pd.DataFrame) -> dict:\n",
    "    scores = {}  # dict of metric name -> metric value/score\n",
    "    \n",
    "    for metric in suitable_metrics:\n",
    "        metric_name = metric.__name__\n",
    "        \n",
    "        score = metric(y_true, y_pred)\n",
    "        \n",
    "        scores[metric_name] = score\n",
    "\n",
    "    return scores\n",
    "\n",
    "def print_scores(scores: dict, model_name: str):\n",
    "  print(f\"\\nScores:\\t{model_name}\\n================================\")\n",
    "  for metric_name, metric_value in scores.items():\n",
    "    print(f\"{metric_name}: {metric_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bff0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models(X_train, y_train, X_test):\n",
    "    # Train Random Forest model with selected features\n",
    "    y_pred_rf, _ = predict_price(X_train, y_train, X_test, RandomForestRegressor)\n",
    "\n",
    "    # Train Linear Regression model with selected features\n",
    "    y_pred_lin, _ = predict_price(X_train, y_train, X_test, LinearRegression)\n",
    "\n",
    "    # Train XGBRegressor model with selected features\n",
    "    y_pred_xgb, _ = predict_price(X_train, y_train, X_test, XGBRegressor)\n",
    "\n",
    "    return y_pred_rf, y_pred_lin, y_pred_xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a5038d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics_evolution(metric_values, model_names, feature_counts, metric_name):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    for i, model_values in enumerate(metric_values):\n",
    "        plt.plot(feature_counts, model_values, label=model_names[i])\n",
    "\n",
    "    plt.title(f'{metric_name} Evolution for different Models')\n",
    "    plt.xlabel('Number of Features')\n",
    "    plt.ylabel(metric_name)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3a934f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance with 10 features\n",
    "y_pred_rf_10, y_pred_lin_10, y_pred_xgb_10 = compare_models(X_train_10_scaled, y_train, X_test_10_scaled)\n",
    "\n",
    "metrics_scores_sel_10 = compare_metrics(y_test, y_pred_rf_10)\n",
    "metrics_scores_lin_10 = compare_metrics(y_test, y_pred_lin_10)\n",
    "metrics_scores_xgb_10 = compare_metrics(y_test, y_pred_xgb_10)\n",
    "\n",
    "print(\"\\nPerformance with 10 features:\")\n",
    "print_scores(metrics_scores_sel_10, model_name='Random Forest')\n",
    "print_scores(metrics_scores_lin_10, model_name='Linear Regression')\n",
    "print_scores(metrics_scores_xgb_10, model_name='XGBRegressor')\n",
    "\n",
    "# Performance with 15 features\n",
    "y_pred_rf_15, y_pred_lin_15, y_pred_xgb_15 = compare_models(X_train_15_scaled, y_train, X_test_15_scaled)\n",
    "\n",
    "metrics_scores_sel_15 = compare_metrics(y_test, y_pred_rf_15)\n",
    "metrics_scores_lin_15 = compare_metrics(y_test, y_pred_lin_15)\n",
    "metrics_scores_xgb_15 = compare_metrics(y_test, y_pred_xgb_15)\n",
    "\n",
    "print(\"\\nPerformance with 15 features:\")\n",
    "print_scores(metrics_scores_sel_15, model_name='Random Forest')\n",
    "print_scores(metrics_scores_lin_15, model_name='Linear Regression')\n",
    "print_scores(metrics_scores_xgb_15, model_name='XGBRegressor')\n",
    "\n",
    "# Performance with 20 features\n",
    "y_pred_rf_20, y_pred_lin_20, y_pred_xgb_20 = compare_models(X_train_20_scaled, y_train, X_test_20_scaled)\n",
    "\n",
    "metrics_scores_sel_20 = compare_metrics(y_test, y_pred_rf_20)\n",
    "metrics_scores_lin_20 = compare_metrics(y_test, y_pred_lin_20)\n",
    "metrics_scores_xgb_20 = compare_metrics(y_test, y_pred_xgb_20)\n",
    "\n",
    "print(\"\\nPerformance with 20 features:\")\n",
    "print_scores(metrics_scores_sel_20, model_name='Random Forest')\n",
    "print_scores(metrics_scores_lin_20, model_name='Linear Regression')\n",
    "print_scores(metrics_scores_xgb_20, model_name='XGBRegressor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126139a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting Metrics Evolution\n",
    "metric_names = ['mean_squared_error', 'mean_absolute_error', 'rmse', 'r2_score']\n",
    "feature_counts = [10, 15, 20]\n",
    "\n",
    "for metric_name in metric_names:\n",
    "    plot_metrics_evolution(\n",
    "        [[metrics_scores_sel_10[metric_name], metrics_scores_sel_15[metric_name], metrics_scores_sel_20[metric_name]],\n",
    "         [metrics_scores_lin_10[metric_name], metrics_scores_lin_15[metric_name], metrics_scores_lin_20[metric_name]],\n",
    "         [metrics_scores_xgb_10[metric_name], metrics_scores_xgb_15[metric_name], metrics_scores_xgb_20[metric_name]]],\n",
    "        ['Random Forest', 'Linear Regression', 'XGBRegressor'],\n",
    "        feature_counts,\n",
    "        metric_name\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6847343",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_actual_vs_predicted(y_true, y_pred, title):\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.scatter(y_true, y_pred, alpha=0.5)\n",
    "    plt.plot([min(y_true), max(y_true)], [min(y_true), max(y_true)], '--k', linewidth=2)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Actual Prices')\n",
    "    plt.ylabel('Predicted Prices')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7dc0c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_actual_vs_predicted(y_test, y_pred_rf_10, 'Random Forest - 10 Features')\n",
    "plot_actual_vs_predicted(y_test, y_pred_lin_10, 'Linear Regression - 10 Features')\n",
    "plot_actual_vs_predicted(y_test, y_pred_xgb_10, 'XGBRegressor - 10 Features')\n",
    "\n",
    "plot_actual_vs_predicted(y_test, y_pred_rf_15, 'Random Forest - 15 Features')\n",
    "plot_actual_vs_predicted(y_test, y_pred_lin_15, 'Linear Regression - 15 Features')\n",
    "plot_actual_vs_predicted(y_test, y_pred_xgb_15, 'XGBRegressor - 15 Features')\n",
    "\n",
    "plot_actual_vs_predicted(y_test, y_pred_rf_20, 'Random Forest - 20 Features')\n",
    "plot_actual_vs_predicted(y_test, y_pred_lin_20, 'Linear Regression - 20 Features')\n",
    "plot_actual_vs_predicted(y_test, y_pred_xgb_20, 'XGBRegressor - 20 Features')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
